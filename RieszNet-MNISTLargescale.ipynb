{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barisin\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from skimage import io, exposure\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import time\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import ifftn, fftn, fft2, ifft2\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxNormalization(tensor):\n",
    "    #min max normalization\n",
    "    v_min, v_max = tensor.min(), tensor.max()\n",
    "    tensor = (tensor - v_min)/(v_max - v_min)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_h5_tr_te_val(n_train, n_test, n_val, filename, \n",
    "                    path):\n",
    "    \"\"\"Data will be in format [n_samples, width, height, n_channels] \"\"\"\n",
    "    \n",
    "    with h5py.File(path + filename, 'r') as f:    \n",
    "        # Data should be floating point\n",
    "        x_train = np.array( f[\"/x_train\"], dtype=np.float32)\n",
    "        x_test = np.array( f[\"/x_test\"], dtype=np.float32)\n",
    "        x_val= np.array( f[\"/x_val\"], dtype=np.float32)\n",
    "\n",
    "        # Labels should normally be integers\n",
    "        y_train = np.array( f[\"/y_train\"], dtype=np.int32)\n",
    "        y_test = np.array( f[\"/y_test\"], dtype=np.int32)\n",
    "        y_val= np.array( f[\"/y_val\"], dtype=np.int32)\n",
    "        \n",
    "        # Labels should normally be 1D vectors, shape (n_labels,) \n",
    "        y_train = np.reshape(y_train,(np.size(y_train),))\n",
    "        y_test = np.reshape(y_test,(np.size(y_test),))\n",
    "        y_val = np.reshape(y_val,(np.size(y_val),))\n",
    "        \n",
    "    # Handle case of data containing only a single sample\n",
    "    # (which is the case for the train and validation partitions in the \"testdata only\" datasets)\n",
    "    if len(np.shape(x_train)) == 3:\n",
    "        x_train = np.expand_dims(x_train, 0)\n",
    "    if len(np.shape(x_test)) == 3:\n",
    "            x_test = np.expand_dims(x_test, 0)\n",
    "    if len(np.shape(x_val)) == 3:\n",
    "        x_val = np.expand_dims(x_val, 0)    \n",
    "        \n",
    "    # Possibly use a different number of samples than in the datasetfile\n",
    "    x_train = x_train[0:n_train]\n",
    "    x_test = x_test[0:n_test]\n",
    "    x_val = x_val[0:n_val]\n",
    "\n",
    "    y_train = y_train[0:n_train]\n",
    "    y_test = y_test[0:n_test]\n",
    "    y_val = y_val[0:n_val]\n",
    "    \n",
    "    assert np.shape(x_train)[0] == n_train\n",
    "    assert np.shape(x_test)[0] == n_test\n",
    "    assert np.shape(x_val)[0] == n_val\n",
    "\n",
    "    assert np.shape(y_train)[0] == n_train\n",
    "    assert np.shape(y_test)[0] == n_test\n",
    "    assert np.shape(y_val)[0] == n_val\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test), (x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RieszNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeRange(size_n):\n",
    "    output = np.zeros(size_n)\n",
    "    center = int(size_n/2)\n",
    "    for i in range(size_n):\n",
    "        output[i] = (i-center)/(size_n - (size_n%2))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RieszDerivatives(img, order):\n",
    "    #get image size\n",
    "    size_x, size_y = img.shape\n",
    "    Riesz_kernel = np.zeros(img.shape, dtype=complex)\n",
    "    #get riesz kernel in fourier domain\n",
    "    gx = MakeRange(size_x)\n",
    "    gy = MakeRange(size_y)\n",
    "    for i in range(size_x):\n",
    "        for j in range(size_y):\n",
    "            den = gx[i]*gx[i]+gy[j]*gy[j]\n",
    "            if den > 1e-08:\n",
    "                if(order[0] == 0 and order[1] == 1):\n",
    "                    den = np.sqrt(den)\n",
    "                    Riesz_kernel[i,j] = complex(0,-gy[j]/den)\n",
    "                if(order[0] == 1 and order[1] == 0):\n",
    "                    den = np.sqrt(den)\n",
    "                    Riesz_kernel[i,j] = complex(0,-gx[i]/den)\n",
    "                if(order[0] == 2 and order[1] == 0):\n",
    "                    Riesz_kernel[i,j] = complex(gx[i]*gx[i]/den,0)\n",
    "                if(order[0] == 1 and order[1] == 1):\n",
    "                    Riesz_kernel[i,j] = complex(gx[i]*gy[j]/den,0)\n",
    "                if(order[0] == 0 and order[1] == 2):\n",
    "                    Riesz_kernel[i,j] = complex(gy[j]*gy[j]/den,0)\n",
    "                    \n",
    "    #fourier transform of the image\n",
    "    fft_img = fft2(img)\n",
    "    fft_img = np.fft.fftshift(fft_img)\n",
    "    result = np.multiply(fft_img, Riesz_kernel)\n",
    "    result = np.fft.ifftshift(result)\n",
    "    result = ifft2(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RieszKernelVar(s_x, s_y, order):\n",
    "    #get riesz kernel in fourier domain\n",
    "    size_x = s_x\n",
    "    size_y = s_y\n",
    "    Riesz_kernel = np.zeros([size_x, size_y], dtype=complex)\n",
    "    gx = MakeRange(size_x)\n",
    "    gy = MakeRange(size_y)\n",
    "    for i in range(size_x):\n",
    "        for j in range(size_y):\n",
    "            den = gx[i]*gx[i]+gy[j]*gy[j]\n",
    "            if den > 1e-08:\n",
    "                if(order[0] == 0 and order[1] == 1):\n",
    "                    den = np.sqrt(den)\n",
    "                    Riesz_kernel[i,j] = complex(0,-gy[j]/den)\n",
    "                if(order[0] == 1 and order[1] == 0):\n",
    "                    den = np.sqrt(den)\n",
    "                    Riesz_kernel[i,j] = complex(0,-gx[i]/den)\n",
    "                if(order[0] == 2 and order[1] == 0):\n",
    "                    Riesz_kernel[i,j] = complex(gx[i]*gx[i]/den,0)\n",
    "                if(order[0] == 1 and order[1] == 1):\n",
    "                    Riesz_kernel[i,j] = complex(gx[i]*gy[j]/den,0)\n",
    "                if(order[0] == 0 and order[1] == 2):\n",
    "                    Riesz_kernel[i,j] = complex(gy[j]*gy[j]/den,0)\n",
    "    Riesz_kernel2 = torch.from_numpy(Riesz_kernel)\n",
    "    Riesz_kernel2= Riesz_kernel2.reshape([1,1,size_x,size_y])\n",
    "    return Riesz_kernel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RieszLayer(nn.Module):\n",
    "    def __init__(self, order):\n",
    "        super(RieszLayer, self).__init__()\n",
    "\n",
    "        self.order = order\n",
    "        self.size_x = 64\n",
    "        self.size_y = 64\n",
    "        self.Riesz_fft = RieszKernelVar(self.size_x,self.size_y, self.order)\n",
    " \n",
    "    def forward(self, x):\n",
    "        #Riesz_fft = RieszKernel(x, self.order)\n",
    "        if x.size()[2] != self.size_x or x.size()[3] != self.size_y:\n",
    "            self.size_x = x.size()[2]\n",
    "            self.size_y = x.size()[3]\n",
    "            self.Riesz_fft = RieszKernelVar(self.size_x, self.size_y, self.order)\n",
    "            \n",
    "\n",
    "        x = x*self.Riesz_fft\n",
    "\n",
    "        return x\n",
    "    \n",
    "class RieszNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RieszNet, self).__init__()\n",
    "        self.order1 = [1, 0]\n",
    "        self.order2 = [0, 1]\n",
    "        self.Rx = RieszLayer(self.order1)\n",
    "        self.Ry = RieszLayer(self.order2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.fft.fft2(x)\n",
    "        x = torch.fft.fftshift(x, dim = [2,3])\n",
    "        x1 = self.Rx(x)\n",
    "        x2 = self.Ry(x)\n",
    "        x3 = -self.Rx(x1)\n",
    "        x4 = -self.Rx(x2)\n",
    "        x5 = -self.Ry(x2)\n",
    " \n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "        x = torch.fft.ifftshift(x, dim = [2,3])\n",
    "        x = torch.real(torch.fft.ifft2(x)).double()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "#12-14-16-20-64 c\n",
    "class RieszNetDeep(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RieszNetDeep, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(5,12,kernel_size = (1,1))\n",
    "        self.conv2 = nn.Conv2d(60,16,kernel_size = (1,1))\n",
    "        self.conv3 = nn.Conv2d(80,24,kernel_size = (1,1))\n",
    "        self.conv4 = nn.Conv2d(120,32,kernel_size = (1,1))\n",
    "        self.conv5 = nn.Conv2d(160,80,kernel_size = (1,1))\n",
    "        self.conv6 = nn.Conv2d(80,10,kernel_size = (1,1))\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(12, affine=True)\n",
    "        self.bn2 = nn.BatchNorm2d(16, affine=True)\n",
    "        self.bn3 = nn.BatchNorm2d(24, affine=True)\n",
    "        self.bn4 = nn.BatchNorm2d(32, affine=True)\n",
    "        self.bn5 = nn.BatchNorm2d(80, affine=True)\n",
    "        \n",
    "        self.GL1 = RieszNet()\n",
    "\n",
    "\n",
    "    def forward(self, y):\n",
    "        \n",
    "        #1st  layer\n",
    "        x = self.GL1(y)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #2nd layer\n",
    "        x = self.GL1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        #3rd layer\n",
    "        x = self.GL1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        #4th layer\n",
    "        x = self.GL1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        #5th layer\n",
    "        x = self.GL1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        #final layer\n",
    "        x = self.conv6(x)\n",
    "        x = F.softmax(x)\n",
    "        x = x[:,:,int(x.shape[2]/2),int(x.shape[2]/2)]\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20882"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LN = RieszNetDeep()\n",
    "\n",
    "LN = LN.double()\n",
    "\n",
    "count_parameters(LN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (50000, 112, 112, 1)\n",
      "Shape testing data:\t (1, 112, 112, 1)\n",
      "Shape validation data:\t (1000, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (50000,)\n",
      "Shape test labels:\t (1,)\n",
      "Shape validation labels:  (1000,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = 'mnist-large/train/7z/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_tr50000_vl10000_te10000_outsize112-112_sctr1p000_scte1p000.h5'\n",
    "\n",
    "n_train = 50000\n",
    "#50000 - max\n",
    "n_val = 1000\n",
    "#10000 - max\n",
    "n_test = 1\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = torch.from_numpy(x_train).float()\n",
    "t1 = t1.reshape([t1.shape[0], 1, t1.shape[1], t1.shape[2]])\n",
    "t2  = torch.from_numpy(y_train).long()\n",
    "t2 = t2.reshape(t2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 112, 112])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1  = torch.from_numpy(x_val).double()\n",
    "val1 = val1.reshape([val1.shape[0], 1, val1.shape[1], val1.shape[2]])\n",
    "val2  = torch.from_numpy(y_val).long()\n",
    "val2 = val2.reshape(val2.shape[0])\n",
    "#val1 = val1[:,:,1:,1:]\n",
    "val1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LN.double()\n",
    "\n",
    "n_epochs = 20 # or whatever\n",
    "batch_size = 50 # or whatever\n",
    "train_num = 50000\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = sched.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "input = t1.double()\n",
    "target = t2#.double()\n",
    "\n",
    "#input_v = test_x2\n",
    "#target_v = test_y2\n",
    "\n",
    "#criterion_val = nn.BCELoss(weight=val_w2)\n",
    "criterion_val = loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    permutation = torch.randperm(train_num)\n",
    "    if(epoch % 2 == 1):\n",
    "        print(epoch)\n",
    "        print(t_loss/train_num*batch_size)\n",
    "        print(\"val:\")\n",
    "        val_loss=0\n",
    "        for i in range(20):\n",
    "            with torch.no_grad():\n",
    "                v1 = model(val1[(i*50):((i+1)*50)])\n",
    "            v2 = val2[(i*50):((i+1)*50)]\n",
    "            #v1 = torch.argmax(v1, dim=1)\n",
    "            val_loss = val_loss + criterion_val(v1,v2)\n",
    "            gc.collect()\n",
    "        print(val_loss/1000*50)\n",
    "        print(\"--------\")\n",
    "    t_loss = 0\n",
    "    gc.collect()\n",
    "   \n",
    "    for i in range(0,train_num, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:(i+batch_size)]\n",
    "        batch_x = input[indices]\n",
    "        batch_y = target[indices]\n",
    "        #batch_w = input_w[indices]\n",
    "        #criterion3 = nn.BCELoss(weight=batch_w)\n",
    "        outputs = model.forward(batch_x)\n",
    "        #print(outputs.shape)\n",
    "        #loss = criterion3(outputs,batch_y)\n",
    "        loss = criterion_val(outputs,batch_y)\n",
    "        \n",
    "        if(epoch % 2 == 0):\n",
    "            t_loss += loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        gc.collect()\n",
    "    \n",
    "    scheduler.step()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"riesznet-mnist-largescale.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y1,y2):\n",
    "    train_acc = torch.sum(y1 == y2)\n",
    "    final_train_acc = train_acc/y2.shape[0]\n",
    "    return final_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_batch(y1,y2):\n",
    "    train_acc = torch.sum(y1 == y2)\n",
    "    return [train_acc.detach().numpy()+0,y2.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LN.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte1p000.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9858, 10000]\n"
     ]
    }
   ],
   "source": [
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9858\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9850, 10000]\n",
      "Accuracy:\n",
      "0.985\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte1p189.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9845, 10000]\n",
      "Accuracy:\n",
      "0.9845\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte1p414.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9840, 10000]\n",
      "Accuracy:\n",
      "0.984\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte1p682.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte2p000.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9839, 10000]\n"
     ]
    }
   ],
   "source": [
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9839\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9824, 10000]\n",
      "Accuracy:\n",
      "0.9824\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte2p378.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9801, 10000]\n",
      "Accuracy:\n",
      "0.9801\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte2p828.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9751, 10000]\n",
      "Accuracy:\n",
      "0.9751\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte3p364.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte4p000.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9642, 10000]\n"
     ]
    }
   ],
   "source": [
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9642\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9350, 10000]\n",
      "Accuracy:\n",
      "0.935\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte4p757.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8158, 10000]\n",
      "Accuracy:\n",
      "0.8158\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte5p657.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6766, 10000]\n",
      "Accuracy:\n",
      "0.6766\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte6p727.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte8p000.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5182, 10000]\n"
     ]
    }
   ],
   "source": [
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5182\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte0p500.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9634, 10000]\n"
     ]
    }
   ],
   "source": [
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9634\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9759, 10000]\n",
      "Accuracy:\n",
      "0.9759\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte0p595.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9806, 10000]\n",
      "Accuracy:\n",
      "0.9806\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte0p707.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training data:\t (1, 112, 112, 1)\n",
      "Shape testing data:\t (10000, 112, 112, 1)\n",
      "Shape validation data:\t (1, 112, 112, 1)\n",
      "\n",
      "Shape training labels:\t (1,)\n",
      "Shape test labels:\t (10000,)\n",
      "Shape validation labels:  (1,)\n",
      "\n",
      "Intensity range: [-0.76, 0.76] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/bv/mikro/home/barisin/Franziska-NN2/TinNN/lib64/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9854, 10000]\n",
      "Accuracy:\n",
      "0.9854\n"
     ]
    }
   ],
   "source": [
    "path = '/mnist-large/'\n",
    "\n",
    "# This specific dataset contains scale variations in the range [1,4] relative the original MNIST dataset\n",
    "filename = 'mnist_large_scale_te10000_outsize112-112_scte0p841.h5'\n",
    "\n",
    "n_train = 1\n",
    "#50000 - max\n",
    "n_val = 1\n",
    "#10000 - max\n",
    "n_test = 10000\n",
    "#10000 - max\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_mnist_h5_tr_te_val(\n",
    "                                                             n_train, n_test, n_val, \n",
    "                                                             filename, path)\n",
    "print(\"Shape training data:\\t\", np.shape(x_train))\n",
    "print(\"Shape testing data:\\t\", np.shape(x_test))\n",
    "print(\"Shape validation data:\\t\", np.shape(x_val))\n",
    "print()\n",
    "print(\"Shape training labels:\\t\", np.shape(y_train))\n",
    "print(\"Shape test labels:\\t\", np.shape(y_test))\n",
    "print(\"Shape validation labels: \", np.shape(y_val))\n",
    "print()\n",
    "print(\"Intensity range: [{:.2f}, {:.2f}] \".format(np.min(x_train), np.max(x_train)))\n",
    "\n",
    "te1  = torch.from_numpy(x_test).float()\n",
    "te1 = te1.reshape([te1.shape[0], 1, te1.shape[1], te1.shape[2]])\n",
    "te2  = torch.from_numpy(y_test).long()\n",
    "te2 = te2.reshape(te2.shape[0])\n",
    "\n",
    "start=[0,0]\n",
    "gc.collect()\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        out_te1 = LN(te1[(i*100):((i+1)*100)])\n",
    "    out_te2 = te2[(i*100):((i+1)*100)]\n",
    "    out_te1 = torch.argmax(out_te1, dim=1)\n",
    "    t = accuracy_batch(out_te1,out_te2)\n",
    "    start[0] = start[0] + t[0]\n",
    "    start[1] = start[1] + t[1]\n",
    "    gc.collect()\n",
    "print(start)\n",
    "print(\"Accuracy:\")\n",
    "print(start[0]/start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
